# ğŸ“¡ Real-Time Data Pipeline (Local-Only Setup)

This project builds a complete **real-time data engineering pipeline** locally using **Docker, Kafka, Spark, and MongoDB**. It's designed for beginners to learn how data flows through modern streaming systems â€” without using any cloud services or incurring costs.

---

## ğŸ”§ Tools Used

- **Docker & Docker Compose** â€“ to containerize and manage services
- **Apache Kafka** â€“ to simulate real-time data streams
- **Apache Spark (Structured Streaming)** â€“ to process data in real time
- **MongoDB** â€“ to store processed data
- **Python** â€“ for producing test data

---

## ğŸ¯ Project Objective

- Simulate real-time sensor or event data (via Kafka Producer)
- Stream that data into Kafka topics
- Use Spark to consume Kafka data, transform it, and store results in MongoDB
- All services run locally using Docker Compose

---

## ğŸ“ Folder Structure

real-time-data-pipeline/
â”œâ”€â”€ .env # Environment variables (hostnames, ports, etc.)
â”œâ”€â”€ docker-compose.yml # Orchestrates all services
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ src/
â”œâ”€â”€ producer/
â”‚ â””â”€â”€ producer.py # Sends fake data to Kafka topic
â”œâ”€â”€ spark/
â”‚ â””â”€â”€ spark_job.py # Spark reads from Kafka & writes to MongoDB
â””â”€â”€ consumer/
â””â”€â”€ consumer.py # (Optional) Reads data from Kafka for testing


---

## ğŸš€ How to Run (Step-by-Step)

1. **Install Prerequisites**  
   - Docker Desktop  
   - Git  
   - Python 3.x (for local scripts)

2. **Clone the Repository**

   ```bash
   git clone https://github.com/your-username/real-time-data-pipeline.git
   cd real-time-data-pipeline

3. **Start All Services**

    This sets up:

        Kafka broker & Zookeeper

        MongoDB

        Spark app

        Python producer (manual or scripted)

ğŸ“Œ Notes
This project runs 100% locally. No cloud credentials needed.

Good for portfolio projects and interview preparation.

You can extend it with dashboards (e.g., using Flask + Chart.js or Superset).


